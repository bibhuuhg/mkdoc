{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SparkBatch's documentattion! Introduction Installation SparkBatch GDPR","title":"Home"},{"location":"#welcome-to-sparkbatchs-documentattion","text":"Introduction Installation SparkBatch GDPR","title":"Welcome to SparkBatch's documentattion!"},{"location":"gdpr/","text":"Access GDPR sensitive columns There are few GDPR senstive columns which are encripted for unauthorized users . Example are houseNumber, StreetName . Using secrete key, Authorized users can access GDPR sensitive columns . Note: We are using encription techniques that will generate uniform value for same key and value . This is extreamly useful if GDPR columns are being used for joining. ```python spark.sql(\"select cast(aes_decrypt(unbase64(address.houseNumber), ' ','ECB') as string) from sales\").show() ``` aes_decrypt: method is being used to decrete the encript value secrete_ley : This key will be shared with authorised users to view GDP columns","title":"GDPR"},{"location":"gdpr/#access-gdpr-sensitive-columns","text":"There are few GDPR senstive columns which are encripted for unauthorized users . Example are houseNumber, StreetName . Using secrete key, Authorized users can access GDPR sensitive columns . Note: We are using encription techniques that will generate uniform value for same key and value . This is extreamly useful if GDPR columns are being used for joining. ```python spark.sql(\"select cast(aes_decrypt(unbase64(address.houseNumber), ' ','ECB') as string) from sales\").show() ``` aes_decrypt: method is being used to decrete the encript value secrete_ley : This key will be shared with authorised users to view GDP columns","title":"Access GDPR sensitive columns"},{"location":"installation/","text":"Installation SparkBatch service can be installed as library into any conda environment . Installation guide as shown below: create a directory $ mkdir -p $HOME/pyspark_service create a log folder for storing apllication logs $ mkdir -p $HOME/logs create a folder for local conda chabbel $ mkdir -p $HOME/conda_channel $ cd $HOME/pyspark_service $ conda activate pyspark_dev_env clone the project $ git clone https://github.com/bibhuuhg/pyspark-service.git conda build $ conda-build . --output-folder $HOME/conda_channel install package $ conda install --channel $HOME/conda_channel sparkbatch","title":"Installation"},{"location":"installation/#installation","text":"SparkBatch service can be installed as library into any conda environment . Installation guide as shown below: create a directory $ mkdir -p $HOME/pyspark_service create a log folder for storing apllication logs $ mkdir -p $HOME/logs create a folder for local conda chabbel $ mkdir -p $HOME/conda_channel $ cd $HOME/pyspark_service $ conda activate pyspark_dev_env clone the project $ git clone https://github.com/bibhuuhg/pyspark-service.git conda build $ conda-build . --output-folder $HOME/conda_channel install package $ conda install --channel $HOME/conda_channel sparkbatch","title":"Installation"},{"location":"introduction/","text":"Introduction PySpark homework assignment Context The goal of this assignment is to get view on your coding workflow & style. Your main focus should be creating performant & robust code for data manipulations. For a homework assignment, we cannot grant you access to our infrastructure (Cloudera data platform on prem: a spark cluster deployment on Yarn). Since the focus is on development, we provided a template notebook to get up and running very quickly on Google Colab. You have the freedom to perform this assignment on any spark3+ infrastructure. If want to use a local or cloud setup, go for it! Some of the tasks are open for interpretation. This allows us to assess business understanding and relevant field experience. These tasks are not pass or fail checks. During the interview we'll ask details about the choice(s) you made. For the assignment, you'll be working with store location data. You might be familiar with the phrase \"Location, location, location\" from the real-estate context. The same house can have a different selling price based on the location. In fast moving consumer goods (FMCG), location is one of the most crucial aspects.","title":"Introduction"},{"location":"introduction/#introduction","text":"PySpark homework assignment Context The goal of this assignment is to get view on your coding workflow & style. Your main focus should be creating performant & robust code for data manipulations. For a homework assignment, we cannot grant you access to our infrastructure (Cloudera data platform on prem: a spark cluster deployment on Yarn). Since the focus is on development, we provided a template notebook to get up and running very quickly on Google Colab. You have the freedom to perform this assignment on any spark3+ infrastructure. If want to use a local or cloud setup, go for it! Some of the tasks are open for interpretation. This allows us to assess business understanding and relevant field experience. These tasks are not pass or fail checks. During the interview we'll ask details about the choice(s) you made. For the assignment, you'll be working with store location data. You might be familiar with the phrase \"Location, location, location\" from the real-estate context. The same house can have a different selling price based on the location. In fast moving consumer goods (FMCG), location is one of the most crucial aspects.","title":"Introduction"},{"location":"sparkbatch/","text":"Usage After installing sparkbatch in your conda environment, there are two steps to use and invoke the packages . Step 1: invoke using CLI. $ python -m sparkbatch --help $ python -m sparkbatch --version $ python -m sparkbatch --input_data_path <input_data_path> --brand <brand_name> --target_dir <output_dir> where input_data_path : input data location brand_name : To fetch record for a brand output_dir : Output location where final result will be stored example : python -m sparkbatch --input_data_path \"user/bibhu/input/\" --brand \"clp\" --target_dir \"/user/bibhu/pyspark_output/\" Step 2: invoke programatically. from sparkbatch.run import get_data_by_brand from sparkbatch.spark_utils import write_df_to_target df = get_data_by_brand(data_path : str, brand: str) write_df_to_target(df : DataFrame, target_dir: str )","title":"SparkBatch"},{"location":"sparkbatch/#usage","text":"After installing sparkbatch in your conda environment, there are two steps to use and invoke the packages .","title":"Usage"},{"location":"sparkbatch/#step-1-invoke-using-cli","text":"$ python -m sparkbatch --help $ python -m sparkbatch --version $ python -m sparkbatch --input_data_path <input_data_path> --brand <brand_name> --target_dir <output_dir> where input_data_path : input data location brand_name : To fetch record for a brand output_dir : Output location where final result will be stored example : python -m sparkbatch --input_data_path \"user/bibhu/input/\" --brand \"clp\" --target_dir \"/user/bibhu/pyspark_output/\"","title":"Step 1: invoke using CLI."},{"location":"sparkbatch/#step-2-invoke-programatically","text":"from sparkbatch.run import get_data_by_brand from sparkbatch.spark_utils import write_df_to_target df = get_data_by_brand(data_path : str, brand: str) write_df_to_target(df : DataFrame, target_dir: str )","title":"Step 2: invoke programatically."}]}